{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVk+Rk5uxImMB4jiUAJlNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaofangZH/Segment-Anything-2-AGV-Test/blob/main/CTPN_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4Z3aqfrSpn6A"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "import os\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "qRAyhuvrqQB0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path='/content/sample_data/ch8_training_localization_transcription_gt_v2.zip'\n",
        "extract_path='/content/sample_data/train_ctpn/train_data/train_label_all/'\n",
        "with zipfile.ZipFile(zip_path) as zip_ref:\n",
        "  zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "WZPmPztzqYZq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# base_dir = 'path to dataset base dir'\n",
        "base_dir = '/content/sample_data/train_ctpn/train_data'\n",
        "img_dir = os.path.join(base_dir, 'train_img')\n",
        "xml_dir = os.path.join(base_dir, 'train_label')\n",
        "\n",
        "icdar17_mlt_img_dir = '/content/sample_data/train_ctpn/train_data/train_img'\n",
        "icdar17_mlt_gt_dir = '/content/sample_data/train_ctpn/train_data/train_label'\n",
        "num_workers = 0\n",
        "pretrained_weights = '/content/sample_data/train_ctpn/checkpoints/v3_ctpn_ep24_0.6899_0.1993_0.8892.pth'\n",
        "\n",
        "\n",
        "\n",
        "anchor_scale = 16\n",
        "IOU_NEGATIVE = 0.3\n",
        "IOU_POSITIVE = 0.7\n",
        "IOU_SELECT = 0.7\n",
        "\n",
        "RPN_POSITIVE_NUM = 150\n",
        "RPN_TOTAL_NUM = 300\n",
        "\n",
        "# bgr can find from  here: https://github.com/fchollet/deep-learning-models/blob/master/imagenet_utils.py\n",
        "IMAGE_MEAN = [123.68, 116.779, 103.939]\n",
        "OHEM = True\n",
        "\n",
        "checkpoints_dir = '/content/sample_data/train_ctpn/checkpoints/'\n",
        "outputs = r'./logs'"
      ],
      "metadata": {
        "id": "No1xv-frq03q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- coding:utf-8 -*-\n",
        "#'''\n",
        "# Created on 18-12-11 上午10:05\n",
        "#\n",
        "# @Author: Greg Gao(laygin)\n",
        "#'''\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
        "    # initialize the dimensions of the image to be resized and\n",
        "    # grab the image size\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    # if both the width and height are None, then return the\n",
        "    # original image\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "\n",
        "    # check to see if the width is None\n",
        "    if width is None:\n",
        "        # calculate the ratio of the height and construct the\n",
        "        # dimensions\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "\n",
        "    # otherwise, the height is None\n",
        "    else:\n",
        "        # calculate the ratio of the width and construct the\n",
        "        # dimensions\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "\n",
        "    # resize the image\n",
        "    resized = cv2.resize(image, dim, interpolation=inter)\n",
        "\n",
        "    # return the resized image\n",
        "    return resized\n",
        "\n",
        "\n",
        "def gen_anchor(featuresize, scale):\n",
        "    \"\"\"\n",
        "        gen base anchor from feature map [HXW][9][4]\n",
        "        reshape  [HXW][9][4] to [HXWX9][4]\n",
        "    \"\"\"\n",
        "    heights = [11, 16, 23, 33, 48, 68, 97, 139, 198, 283]\n",
        "    widths = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
        "\n",
        "    # gen k=9 anchor size (h,w)\n",
        "    heights = np.array(heights).reshape(len(heights), 1)\n",
        "    widths = np.array(widths).reshape(len(widths), 1)\n",
        "\n",
        "    base_anchor = np.array([0, 0, 15, 15])\n",
        "    # center x,y\n",
        "    xt = (base_anchor[0] + base_anchor[2]) * 0.5\n",
        "    yt = (base_anchor[1] + base_anchor[3]) * 0.5\n",
        "\n",
        "    # x1 y1 x2 y2\n",
        "    x1 = xt - widths * 0.5\n",
        "    y1 = yt - heights * 0.5\n",
        "    x2 = xt + widths * 0.5\n",
        "    y2 = yt + heights * 0.5\n",
        "    base_anchor = np.hstack((x1, y1, x2, y2))\n",
        "\n",
        "    h, w = featuresize\n",
        "    shift_x = np.arange(0, w) * scale\n",
        "    shift_y = np.arange(0, h) * scale\n",
        "    # apply shift\n",
        "    anchor = []\n",
        "    for i in shift_y:\n",
        "        for j in shift_x:\n",
        "            anchor.append(base_anchor + [j, i, j, i])\n",
        "    return np.array(anchor).reshape((-1, 4))\n",
        "\n",
        "\n",
        "def cal_iou(box1, box1_area , boxes2, boxes2_area):\n",
        "    \"\"\"\n",
        "    box1 [x1,y1,x2,y2]\n",
        "    boxes2 [Msample,x1,y1,x2,y2]\n",
        "    \"\"\"\n",
        "    x1 = np.maximum(box1[0], boxes2[:, 0])\n",
        "    x2 = np.minimum(box1[2], boxes2[:, 2])\n",
        "    y1 = np.maximum(box1[1], boxes2[:, 1])\n",
        "    y2 = np.minimum(box1[3], boxes2[:, 3])\n",
        "\n",
        "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
        "    iou = intersection / (box1_area + boxes2_area[:] - intersection[:])\n",
        "    return iou\n",
        "\n",
        "\n",
        "def cal_overlaps(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    boxes1 [Nsample,x1,y1,x2,y2]  anchor\n",
        "    boxes2 [Msample,x1,y1,x2,y2]  grouth-box\n",
        "\n",
        "    \"\"\"\n",
        "    area1 = (boxes1[:, 0] - boxes1[:, 2]) * (boxes1[:, 1] - boxes1[:, 3])\n",
        "    area2 = (boxes2[:, 0] - boxes2[:, 2]) * (boxes2[:, 1] - boxes2[:, 3])\n",
        "\n",
        "    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n",
        "\n",
        "    # calculate the intersection of  boxes1(anchor) and boxes2(GT box)\n",
        "    for i in range(boxes1.shape[0]):\n",
        "        overlaps[i][:] = cal_iou(boxes1[i], area1[i], boxes2, area2)\n",
        "\n",
        "    return overlaps\n",
        "\n",
        "\n",
        "def bbox_transfrom(anchors, gtboxes):\n",
        "    \"\"\"\n",
        "     compute relative predicted vertical coordinates Vc ,Vh\n",
        "        with respect to the bounding box location of an anchor\n",
        "    \"\"\"\n",
        "    regr = np.zeros((anchors.shape[0], 2))\n",
        "    Cy = (gtboxes[:, 1] + gtboxes[:, 3]) * 0.5\n",
        "    Cya = (anchors[:, 1] + anchors[:, 3]) * 0.5\n",
        "    h = gtboxes[:, 3] - gtboxes[:, 1] + 1.0\n",
        "    ha = anchors[:, 3] - anchors[:, 1] + 1.0\n",
        "\n",
        "    Vc = (Cy - Cya) / ha\n",
        "    Vh = np.log(h / ha)\n",
        "\n",
        "    return np.vstack((Vc, Vh)).transpose()\n",
        "\n",
        "\n",
        "def bbox_transfor_inv(anchor, regr):\n",
        "    \"\"\"\n",
        "        return predict bbox\n",
        "    \"\"\"\n",
        "\n",
        "    Cya = (anchor[:, 1] + anchor[:, 3]) * 0.5\n",
        "    ha = anchor[:, 3] - anchor[:, 1] + 1\n",
        "\n",
        "    Vcx = regr[0, :, 0]\n",
        "    Vhx = regr[0, :, 1]\n",
        "\n",
        "    Cyx = Vcx * ha + Cya\n",
        "    hx = np.exp(Vhx) * ha\n",
        "    xt = (anchor[:, 0] + anchor[:, 2]) * 0.5\n",
        "\n",
        "    x1 = xt - 16 * 0.5\n",
        "    y1 = Cyx - hx * 0.5\n",
        "    x2 = xt + 16 * 0.5\n",
        "    y2 = Cyx + hx * 0.5\n",
        "    bbox = np.vstack((x1, y1, x2, y2)).transpose()\n",
        "\n",
        "    return bbox\n",
        "\n",
        "\n",
        "def clip_box(bbox, im_shape):\n",
        "    # x1 >= 0\n",
        "    bbox[:, 0] = np.maximum(np.minimum(bbox[:, 0], im_shape[1] - 1), 0)\n",
        "    # y1 >= 0\n",
        "    bbox[:, 1] = np.maximum(np.minimum(bbox[:, 1], im_shape[0] - 1), 0)\n",
        "    # x2 < im_shape[1]\n",
        "    bbox[:, 2] = np.maximum(np.minimum(bbox[:, 2], im_shape[1] - 1), 0)\n",
        "    # y2 < im_shape[0]\n",
        "    bbox[:, 3] = np.maximum(np.minimum(bbox[:, 3], im_shape[0] - 1), 0)\n",
        "\n",
        "    return bbox\n",
        "\n",
        "\n",
        "def filter_bbox(bbox, minsize):\n",
        "    ws = bbox[:, 2] - bbox[:, 0] + 1\n",
        "    hs = bbox[:, 3] - bbox[:, 1] + 1\n",
        "    keep = np.where((ws >= minsize) & (hs >= minsize))[0]\n",
        "    return keep\n",
        "\n",
        "\n",
        "def cal_rpn(imgsize, featuresize, scale, gtboxes):\n",
        "    imgh, imgw = imgsize\n",
        "\n",
        "    # gen base anchor\n",
        "    base_anchor = gen_anchor(featuresize, scale)\n",
        "\n",
        "    # calculate iou\n",
        "    overlaps = cal_overlaps(base_anchor, gtboxes)\n",
        "\n",
        "    # init labels -1 don't care  0 is negative  1 is positive\n",
        "    labels = np.empty(base_anchor.shape[0])\n",
        "    labels.fill(-1)\n",
        "\n",
        "    # for each GT box corresponds to an anchor which has highest IOU\n",
        "    gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
        "\n",
        "    # the anchor with the highest IOU overlap with a GT box\n",
        "    anchor_argmax_overlaps = overlaps.argmax(axis=1)\n",
        "    anchor_max_overlaps = overlaps[range(overlaps.shape[0]), anchor_argmax_overlaps]\n",
        "\n",
        "    # IOU > IOU_POSITIVE\n",
        "    labels[anchor_max_overlaps > IOU_POSITIVE] = 1\n",
        "    # IOU <IOU_NEGATIVE\n",
        "    labels[anchor_max_overlaps < IOU_NEGATIVE] = 0\n",
        "    # ensure that every GT box has at least one positive RPN region\n",
        "    labels[gt_argmax_overlaps] = 1\n",
        "\n",
        "    # only keep anchors inside the image\n",
        "    outside_anchor = np.where(\n",
        "        (base_anchor[:, 0] < 0) |\n",
        "        (base_anchor[:, 1] < 0) |\n",
        "        (base_anchor[:, 2] >= imgw) |\n",
        "        (base_anchor[:, 3] >= imgh)\n",
        "    )[0]\n",
        "    labels[outside_anchor] = -1\n",
        "\n",
        "    # subsample positive labels ,if greater than RPN_POSITIVE_NUM(default 128)\n",
        "    fg_index = np.where(labels == 1)[0]\n",
        "    # print(len(fg_index))\n",
        "    if (len(fg_index) > RPN_POSITIVE_NUM):\n",
        "        labels[np.random.choice(fg_index, len(fg_index) - RPN_POSITIVE_NUM, replace=False)] = -1\n",
        "\n",
        "    # subsample negative labels\n",
        "    if not OHEM:\n",
        "        bg_index = np.where(labels == 0)[0]\n",
        "        num_bg = RPN_TOTAL_NUM - np.sum(labels == 1)\n",
        "        if (len(bg_index) > num_bg):\n",
        "            # print('bgindex:',len(bg_index),'num_bg',num_bg)\n",
        "            labels[np.random.choice(bg_index, len(bg_index) - num_bg, replace=False)] = -1\n",
        "\n",
        "    # calculate bbox targets\n",
        "    # debug here\n",
        "    bbox_targets = bbox_transfrom(base_anchor, gtboxes[anchor_argmax_overlaps, :])\n",
        "    # bbox_targets=[]\n",
        "    # print(len(labels),len(bbox_targets),len(base_anchor),base_anchor[0],labels[0])\n",
        "\n",
        "    return [labels, bbox_targets], base_anchor\n",
        "\n",
        "\n",
        "def nms(dets, thresh):\n",
        "    x1 = dets[:, 0]\n",
        "    y1 = dets[:, 1]\n",
        "    x2 = dets[:, 2]\n",
        "    y2 = dets[:, 3]\n",
        "    scores = dets[:, 4]\n",
        "\n",
        "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "        inds = np.where(ovr <= thresh)[0]\n",
        "        order = order[inds + 1]\n",
        "    return keep\n",
        "\n",
        "\n",
        "# for predict\n",
        "class Graph:\n",
        "    def __init__(self, graph):\n",
        "        self.graph = graph\n",
        "\n",
        "    def sub_graphs_connected(self):\n",
        "        sub_graphs = []\n",
        "        for index in range(self.graph.shape[0]):\n",
        "            if not self.graph[:, index].any() and self.graph[index, :].any():\n",
        "                v = index\n",
        "                sub_graphs.append([v])\n",
        "                while self.graph[v, :].any():\n",
        "                    v = np.where(self.graph[v, :])[0][0]\n",
        "                    sub_graphs[-1].append(v)\n",
        "        return sub_graphs\n",
        "\n",
        "\n",
        "class TextLineCfg:\n",
        "    SCALE = 600\n",
        "    MAX_SCALE = 1200\n",
        "    TEXT_PROPOSALS_WIDTH = 16\n",
        "    MIN_NUM_PROPOSALS = 2\n",
        "    MIN_RATIO = 0.5\n",
        "    LINE_MIN_SCORE = 0.9\n",
        "    MAX_HORIZONTAL_GAP = 60\n",
        "    TEXT_PROPOSALS_MIN_SCORE = 0.7\n",
        "    TEXT_PROPOSALS_NMS_THRESH = 0.3\n",
        "    MIN_V_OVERLAPS = 0.6\n",
        "    MIN_SIZE_SIM = 0.6\n",
        "\n",
        "\n",
        "class TextProposalGraphBuilder:\n",
        "    \"\"\"\n",
        "        Build Text proposals into a graph.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_successions(self, index):\n",
        "        box = self.text_proposals[index]\n",
        "        results = []\n",
        "        for left in range(int(box[0]) + 1, min(int(box[0]) + TextLineCfg.MAX_HORIZONTAL_GAP + 1, self.im_size[1])):\n",
        "            adj_box_indices = self.boxes_table[left]\n",
        "            for adj_box_index in adj_box_indices:\n",
        "                if self.meet_v_iou(adj_box_index, index):\n",
        "                    results.append(adj_box_index)\n",
        "            if len(results) != 0:\n",
        "                return results\n",
        "        return results\n",
        "\n",
        "    def get_precursors(self, index):\n",
        "        box = self.text_proposals[index]\n",
        "        results = []\n",
        "        for left in range(int(box[0]) - 1, max(int(box[0] - TextLineCfg.MAX_HORIZONTAL_GAP), 0) - 1, -1):\n",
        "            adj_box_indices = self.boxes_table[left]\n",
        "            for adj_box_index in adj_box_indices:\n",
        "                if self.meet_v_iou(adj_box_index, index):\n",
        "                    results.append(adj_box_index)\n",
        "            if len(results) != 0:\n",
        "                return results\n",
        "        return results\n",
        "\n",
        "    def is_succession_node(self, index, succession_index):\n",
        "        precursors = self.get_precursors(succession_index)\n",
        "        if self.scores[index] >= np.max(self.scores[precursors]):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def meet_v_iou(self, index1, index2):\n",
        "        def overlaps_v(index1, index2):\n",
        "            h1 = self.heights[index1]\n",
        "            h2 = self.heights[index2]\n",
        "            y0 = max(self.text_proposals[index2][1], self.text_proposals[index1][1])\n",
        "            y1 = min(self.text_proposals[index2][3], self.text_proposals[index1][3])\n",
        "            return max(0, y1 - y0 + 1) / min(h1, h2)\n",
        "\n",
        "        def size_similarity(index1, index2):\n",
        "            h1 = self.heights[index1]\n",
        "            h2 = self.heights[index2]\n",
        "            return min(h1, h2) / max(h1, h2)\n",
        "\n",
        "        return overlaps_v(index1, index2) >= TextLineCfg.MIN_V_OVERLAPS and \\\n",
        "               size_similarity(index1, index2) >= TextLineCfg.MIN_SIZE_SIM\n",
        "\n",
        "    def build_graph(self, text_proposals, scores, im_size):\n",
        "        self.text_proposals = text_proposals\n",
        "        self.scores = scores\n",
        "        self.im_size = im_size\n",
        "        self.heights = text_proposals[:, 3] - text_proposals[:, 1] + 1\n",
        "\n",
        "        boxes_table = [[] for _ in range(self.im_size[1])]\n",
        "        for index, box in enumerate(text_proposals):\n",
        "            boxes_table[int(box[0])].append(index)\n",
        "        self.boxes_table = boxes_table\n",
        "\n",
        "        graph = np.zeros((text_proposals.shape[0], text_proposals.shape[0]), np.bool)\n",
        "\n",
        "        for index, box in enumerate(text_proposals):\n",
        "            successions = self.get_successions(index)\n",
        "            if len(successions) == 0:\n",
        "                continue\n",
        "            succession_index = successions[np.argmax(scores[successions])]\n",
        "            if self.is_succession_node(index, succession_index):\n",
        "                # NOTE: a box can have multiple successions(precursors) if multiple successions(precursors)\n",
        "                # have equal scores.\n",
        "                graph[index, succession_index] = True\n",
        "        return Graph(graph)\n",
        "\n",
        "\n",
        "class TextProposalConnectorOriented:\n",
        "    \"\"\"\n",
        "        Connect text proposals into text lines\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.graph_builder = TextProposalGraphBuilder()\n",
        "\n",
        "    def group_text_proposals(self, text_proposals, scores, im_size):\n",
        "        graph = self.graph_builder.build_graph(text_proposals, scores, im_size)\n",
        "        return graph.sub_graphs_connected()\n",
        "\n",
        "    def fit_y(self, X, Y, x1, x2):\n",
        "        # len(X) != 0\n",
        "        # if X only include one point, the function will get line y=Y[0]\n",
        "        if np.sum(X == X[0]) == len(X):\n",
        "            return Y[0], Y[0]\n",
        "        p = np.poly1d(np.polyfit(X, Y, 1))\n",
        "        return p(x1), p(x2)\n",
        "\n",
        "    def get_text_lines(self, text_proposals, scores, im_size):\n",
        "        \"\"\"\n",
        "        text_proposals:boxes\n",
        "\n",
        "        \"\"\"\n",
        "        # tp=text proposal\n",
        "        tp_groups = self.group_text_proposals(text_proposals, scores, im_size)  # 首先还是建图，获取到文本行由哪几个小框构成\n",
        "\n",
        "        text_lines = np.zeros((len(tp_groups), 8), np.float32)\n",
        "\n",
        "        for index, tp_indices in enumerate(tp_groups):\n",
        "            text_line_boxes = text_proposals[list(tp_indices)]  # 每个文本行的全部小框\n",
        "            X = (text_line_boxes[:, 0] + text_line_boxes[:, 2]) / 2  # 求每一个小框的中心x，y坐标\n",
        "            Y = (text_line_boxes[:, 1] + text_line_boxes[:, 3]) / 2\n",
        "\n",
        "            z1 = np.polyfit(X, Y, 1)  # 多项式拟合，根据之前求的中心店拟合一条直线（最小二乘）\n",
        "\n",
        "            x0 = np.min(text_line_boxes[:, 0])  # 文本行x坐标最小值\n",
        "            x1 = np.max(text_line_boxes[:, 2])  # 文本行x坐标最大值\n",
        "\n",
        "            offset = (text_line_boxes[0, 2] - text_line_boxes[0, 0]) * 0.5  # 小框宽度的一半\n",
        "\n",
        "            # 以全部小框的左上角这个点去拟合一条直线，然后计算一下文本行x坐标的极左极右对应的y坐标\n",
        "            lt_y, rt_y = self.fit_y(text_line_boxes[:, 0], text_line_boxes[:, 1], x0 + offset, x1 - offset)\n",
        "            # 以全部小框的左下角这个点去拟合一条直线，然后计算一下文本行x坐标的极左极右对应的y坐标\n",
        "            lb_y, rb_y = self.fit_y(text_line_boxes[:, 0], text_line_boxes[:, 3], x0 + offset, x1 - offset)\n",
        "\n",
        "            score = scores[list(tp_indices)].sum() / float(len(tp_indices))  # 求全部小框得分的均值作为文本行的均值\n",
        "\n",
        "            text_lines[index, 0] = x0\n",
        "            text_lines[index, 1] = min(lt_y, rt_y)  # 文本行上端 线段 的y坐标的小值\n",
        "            text_lines[index, 2] = x1\n",
        "            text_lines[index, 3] = max(lb_y, rb_y)  # 文本行下端 线段 的y坐标的大值\n",
        "            text_lines[index, 4] = score  # 文本行得分\n",
        "            text_lines[index, 5] = z1[0]  # 根据中心点拟合的直线的k，b\n",
        "            text_lines[index, 6] = z1[1]\n",
        "            height = np.mean((text_line_boxes[:, 3] - text_line_boxes[:, 1]))  # 小框平均高度\n",
        "            text_lines[index, 7] = height + 2.5\n",
        "\n",
        "        text_recs = np.zeros((len(text_lines), 9), np.float)\n",
        "        index = 0\n",
        "        for line in text_lines:\n",
        "            b1 = line[6] - line[7] / 2  # 根据高度和文本行中心线，求取文本行上下两条线的b值\n",
        "            b2 = line[6] + line[7] / 2\n",
        "            x1 = line[0]\n",
        "            y1 = line[5] * line[0] + b1  # 左上\n",
        "            x2 = line[2]\n",
        "            y2 = line[5] * line[2] + b1  # 右上\n",
        "            x3 = line[0]\n",
        "            y3 = line[5] * line[0] + b2  # 左下\n",
        "            x4 = line[2]\n",
        "            y4 = line[5] * line[2] + b2  # 右下\n",
        "            disX = x2 - x1\n",
        "            disY = y2 - y1\n",
        "            width = np.sqrt(disX * disX + disY * disY)  # 文本行宽度\n",
        "\n",
        "            fTmp0 = y3 - y1  # 文本行高度\n",
        "            fTmp1 = fTmp0 * disY / width\n",
        "            x = np.fabs(fTmp1 * disX / width)  # 做补偿\n",
        "            y = np.fabs(fTmp1 * disY / width)\n",
        "            if line[5] < 0:\n",
        "                x1 -= x\n",
        "                y1 += y\n",
        "                x4 += x\n",
        "                y4 -= y\n",
        "            else:\n",
        "                x2 += x\n",
        "                y2 += y\n",
        "                x3 -= x\n",
        "                y3 -= y\n",
        "            text_recs[index, 0] = x1\n",
        "            text_recs[index, 1] = y1\n",
        "            text_recs[index, 2] = x2\n",
        "            text_recs[index, 3] = y2\n",
        "            text_recs[index, 4] = x3\n",
        "            text_recs[index, 5] = y3\n",
        "            text_recs[index, 6] = x4\n",
        "            text_recs[index, 7] = y4\n",
        "            text_recs[index, 8] = line[4]\n",
        "            index = index + 1\n",
        "\n",
        "        return text_recs\n",
        "\n"
      ],
      "metadata": {
        "id": "sf6p3yXUrSgo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- coding:utf-8 -*-\n",
        "#'''\n",
        "# Created on 18-12-11 上午10:01\n",
        "#\n",
        "# @Author: Greg Gao(laygin)\n",
        "#'''\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RPN_REGR_Loss(nn.Module):\n",
        "    def __init__(self, device, sigma=9.0):\n",
        "        super(RPN_REGR_Loss, self).__init__()\n",
        "        self.sigma = sigma\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        '''\n",
        "        smooth L1 loss\n",
        "        :param input:y_preds\n",
        "        :param target: y_true\n",
        "        :return:\n",
        "        '''\n",
        "        try:\n",
        "            cls = target[0, :, 0]\n",
        "            regr = target[0, :, 1:3]\n",
        "            # apply regression to positive sample\n",
        "            regr_keep = (cls == 1).nonzero()[:, 0]\n",
        "            regr_true = regr[regr_keep]\n",
        "            regr_pred = input[0][regr_keep]\n",
        "            diff = torch.abs(regr_true - regr_pred)\n",
        "            less_one = (diff<1.0/self.sigma).float()\n",
        "            loss = less_one * 0.5 * diff ** 2 * self.sigma + torch.abs(1- less_one) * (diff - 0.5/self.sigma)\n",
        "            loss = torch.sum(loss, 1)\n",
        "            loss = torch.mean(loss) if loss.numel() > 0 else torch.tensor(0.0)\n",
        "        except Exception as e:\n",
        "            print('RPN_REGR_Loss Exception:', e)\n",
        "            # print(input, target)\n",
        "            loss = torch.tensor(0.0)\n",
        "\n",
        "        return loss.to(self.device)\n",
        "\n",
        "\n",
        "class RPN_CLS_Loss(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super(RPN_CLS_Loss, self).__init__()\n",
        "        self.device = device\n",
        "        self.L_cls = nn.CrossEntropyLoss(reduction='none')\n",
        "        # self.L_regr = nn.SmoothL1Loss()\n",
        "        # self.L_refi = nn.SmoothL1Loss()\n",
        "        self.pos_neg_ratio = 3\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if OHEM:\n",
        "            cls_gt = target[0][0]\n",
        "            num_pos = 0\n",
        "            loss_pos_sum = 0\n",
        "\n",
        "            # print(len((cls_gt == 0).nonzero()),len((cls_gt == 1).nonzero()))\n",
        "\n",
        "            if len((cls_gt == 1).nonzero())!=0:       # avoid num of pos sample is 0\n",
        "                cls_pos = (cls_gt == 1).nonzero()[:, 0]\n",
        "                gt_pos = cls_gt[cls_pos].long()\n",
        "                cls_pred_pos = input[0][cls_pos]\n",
        "                # print(cls_pred_pos.shape)\n",
        "                loss_pos = self.L_cls(cls_pred_pos.view(-1, 2), gt_pos.view(-1))\n",
        "                loss_pos_sum = loss_pos.sum()\n",
        "                num_pos = len(loss_pos)\n",
        "\n",
        "            cls_neg = (cls_gt == 0).nonzero()[:, 0]\n",
        "            gt_neg = cls_gt[cls_neg].long()\n",
        "            cls_pred_neg = input[0][cls_neg]\n",
        "\n",
        "            loss_neg = self.L_cls(cls_pred_neg.view(-1, 2), gt_neg.view(-1))\n",
        "            loss_neg_topK, _ = torch.topk(loss_neg, min(len(loss_neg), RPN_TOTAL_NUM-num_pos))\n",
        "            loss_cls = loss_pos_sum+loss_neg_topK.sum()\n",
        "            loss_cls = loss_cls/RPN_TOTAL_NUM\n",
        "            return loss_cls.to(self.device)\n",
        "        else:\n",
        "            y_true = target[0][0]\n",
        "            cls_keep = (y_true != -1).nonzero()[:, 0]\n",
        "            cls_true = y_true[cls_keep].long()\n",
        "            cls_pred = input[0][cls_keep]\n",
        "            loss = F.nll_loss(F.log_softmax(cls_pred, dim=-1),\n",
        "                              cls_true)  # original is sparse_softmax_cross_entropy_with_logits\n",
        "            # loss = nn.BCEWithLogitsLoss()(cls_pred[:,0], cls_true.float())  # 18-12-8\n",
        "            loss = torch.clamp(torch.mean(loss), 0, 10) if loss.numel() > 0 else torch.tensor(0.0)\n",
        "            return loss.to(self.device)\n",
        "\n",
        "\n",
        "class basic_conv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_planes,\n",
        "                 out_planes,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 groups=1,\n",
        "                 relu=True,\n",
        "                 bn=True,\n",
        "                 bias=True):\n",
        "        super(basic_conv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CTPN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base_model = models.vgg16(pretrained=False)\n",
        "        layers = list(base_model.features)[:-1]\n",
        "        self.base_layers = nn.Sequential(*layers)  # block5_conv3 output\n",
        "        self.rpn = basic_conv(512, 512, 3, 1, 1, bn=False)\n",
        "        self.brnn = nn.GRU(512,128, bidirectional=True, batch_first=True)\n",
        "        self.lstm_fc = basic_conv(256, 512, 1, 1, relu=True, bn=False)\n",
        "        self.rpn_class = basic_conv(512, 10 * 2, 1, 1, relu=False, bn=False)\n",
        "        self.rpn_regress = basic_conv(512, 10 * 2, 1, 1, relu=False, bn=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print ('1:',x.size())\n",
        "        x = self.base_layers(x)\n",
        "        #print ('2:',x.size())\n",
        "        # rpn\n",
        "        x = self.rpn(x)    #[b, c, h, w]\n",
        "        #print ('3:',x.size())\n",
        "        x1 = x.permute(0,2,3,1).contiguous()  # channels last   [b, h, w, c]\n",
        "        #print ('4:',x1.size())\n",
        "        b = x1.size()  # b, h, w, c\n",
        "        x1 = x1.view(b[0]*b[1], b[2], b[3])\n",
        "        #print ('5:',x1.size())\n",
        "        x2, _ = self.brnn(x1)\n",
        "        #print ('6:',x2.size())\n",
        "        xsz = x.size()\n",
        "        x3 = x2.view(xsz[0], xsz[2], xsz[3], 256)  # torch.Size([4, 20, 20, 256])\n",
        "        #print ('7:',x3.size())\n",
        "        x3 = x3.permute(0,3,1,2).contiguous()  # channels first [b, c, h, w]\n",
        "        #print ('8:',x3.size())\n",
        "        x3 = self.lstm_fc(x3)\n",
        "        #print ('9:',x3.size())\n",
        "        x = x3\n",
        "\n",
        "        cls = self.rpn_class(x)\n",
        "        #print ('10:',cls.size())\n",
        "        regr = self.rpn_regress(x)\n",
        "        #print ('11:',regr.size())\n",
        "        cls = cls.permute(0,2,3,1).contiguous()\n",
        "        regr = regr.permute(0,2,3,1).contiguous()\n",
        "\n",
        "        cls = cls.view(cls.size(0), cls.size(1)*cls.size(2)*10, 2)\n",
        "        #print ('12:',cls.size())\n",
        "        regr = regr.view(regr.size(0), regr.size(1)*regr.size(2)*10, 2)\n",
        "        #print ('13:',regr.size())\n",
        "\n",
        "        return cls, regr\n"
      ],
      "metadata": {
        "id": "i4ItYou2rfVA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- coding:utf-8 -*-\n",
        "#'''\n",
        "# Created on 18-12-27 上午10:34\n",
        "#\n",
        "# @Author: Greg Gao(laygin)\n",
        "#'''\n",
        "\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "def readxml(path):\n",
        "    gtboxes = []\n",
        "    imgfile = ''\n",
        "    xml = ET.parse(path)\n",
        "    for elem in xml.iter():\n",
        "        if 'filename' in elem.tag:\n",
        "            imgfile = elem.text\n",
        "        if 'object' in elem.tag:\n",
        "            for attr in list(elem):\n",
        "                if 'bndbox' in attr.tag:\n",
        "                    xmin = int(round(float(attr.find('xmin').text)))\n",
        "                    ymin = int(round(float(attr.find('ymin').text)))\n",
        "                    xmax = int(round(float(attr.find('xmax').text)))\n",
        "                    ymax = int(round(float(attr.find('ymax').text)))\n",
        "\n",
        "                    gtboxes.append((xmin, ymin, xmax, ymax))\n",
        "\n",
        "    return np.array(gtboxes), imgfile\n",
        "\n",
        "\n",
        "# for ctpn text detection\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 datadir,\n",
        "                 labelsdir):\n",
        "        '''\n",
        "\n",
        "        :param txtfile: image name list text file\n",
        "        :param datadir: image's directory\n",
        "        :param labelsdir: annotations' directory\n",
        "        '''\n",
        "        if not os.path.isdir(datadir):\n",
        "            raise Exception('[ERROR] {} is not a directory'.format(datadir))\n",
        "        if not os.path.isdir(labelsdir):\n",
        "            raise Exception('[ERROR] {} is not a directory'.format(labelsdir))\n",
        "\n",
        "        self.datadir = datadir\n",
        "        self.img_names = os.listdir(self.datadir)\n",
        "        self.labelsdir = labelsdir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img_path = os.path.join(self.datadir, img_name)\n",
        "        print(img_path)\n",
        "        xml_path = os.path.join(self.labelsdir, img_name.replace('.jpg', '.xml'))\n",
        "        gtbox, _ = readxml(xml_path)\n",
        "        img = cv2.imread(img_path)\n",
        "        h, w, c = img.shape\n",
        "\n",
        "        # clip image\n",
        "        if np.random.randint(2) == 1:\n",
        "            img = img[:, ::-1, :]\n",
        "            newx1 = w - gtbox[:, 2] - 1\n",
        "            newx2 = w - gtbox[:, 0] - 1\n",
        "            gtbox[:, 0] = newx1\n",
        "            gtbox[:, 2] = newx2\n",
        "\n",
        "        [cls, regr], _ = cal_rpn((h, w), (int(h / 16), int(w / 16)), 16, gtbox)\n",
        "\n",
        "        m_img = img - IMAGE_MEAN\n",
        "\n",
        "        regr = np.hstack([cls.reshape(cls.shape[0], 1), regr])\n",
        "\n",
        "        cls = np.expand_dims(cls, axis=0)\n",
        "\n",
        "        # transform to torch tensor\n",
        "        m_img = torch.from_numpy(m_img.transpose([2, 0, 1])).float()\n",
        "        cls = torch.from_numpy(cls).float()\n",
        "        regr = torch.from_numpy(regr).float()\n",
        "\n",
        "        return m_img, cls, regr\n",
        "\n",
        "class ICDARDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 datadir,\n",
        "                 labelsdir):\n",
        "        '''\n",
        "\n",
        "        :param txtfile: image name list text file\n",
        "        :param datadir: image's directory\n",
        "        :param labelsdir: annotations' directory\n",
        "        '''\n",
        "        if not os.path.isdir(datadir):\n",
        "            raise Exception('[ERROR] {} is not a directory'.format(datadir))\n",
        "        if not os.path.isdir(labelsdir):\n",
        "            raise Exception('[ERROR] {} is not a directory'.format(labelsdir))\n",
        "\n",
        "        self.datadir = datadir\n",
        "        self.img_names = os.listdir(self.datadir)\n",
        "        self.labelsdir = labelsdir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def box_transfer(self,coor_lists,rescale_fac = 1.0):\n",
        "        gtboxes = []\n",
        "        for coor_list in coor_lists:\n",
        "            coors_x = [int(coor_list[2*i]) for i in range(4)]\n",
        "            coors_y = [int(coor_list[2*i+1]) for i in range(4)]\n",
        "            xmin = min(coors_x)\n",
        "            xmax = max(coors_x)\n",
        "            ymin = min(coors_y)\n",
        "            ymax = max(coors_y)\n",
        "            if rescale_fac>1.0:\n",
        "                xmin = int(xmin / rescale_fac)\n",
        "                xmax = int(xmax / rescale_fac)\n",
        "                ymin = int(ymin / rescale_fac)\n",
        "                ymax = int(ymax / rescale_fac)\n",
        "            gtboxes.append((xmin, ymin, xmax, ymax))\n",
        "        return np.array(gtboxes)\n",
        "\n",
        "    def box_transfer_v2(self,coor_lists,rescale_fac = 1.0):\n",
        "        gtboxes = []\n",
        "        for coor_list in coor_lists:\n",
        "            coors_x = [int(coor_list[2 * i]) for i in range(4)]\n",
        "            coors_y = [int(coor_list[2 * i + 1]) for i in range(4)]\n",
        "            xmin = min(coors_x)\n",
        "            xmax = max(coors_x)\n",
        "            ymin = min(coors_y)\n",
        "            ymax = max(coors_y)\n",
        "            if rescale_fac > 1.0:\n",
        "                xmin = int(xmin / rescale_fac)\n",
        "                xmax = int(xmax / rescale_fac)\n",
        "                ymin = int(ymin / rescale_fac)\n",
        "                ymax = int(ymax / rescale_fac)\n",
        "            prev = xmin\n",
        "            for i in range(xmin // 16 + 1, xmax // 16 + 1):\n",
        "                next = 16*i-0.5\n",
        "                gtboxes.append((prev, ymin, next, ymax))\n",
        "                prev = next\n",
        "            gtboxes.append((prev, ymin, xmax, ymax))\n",
        "        return np.array(gtboxes)\n",
        "\n",
        "    def parse_gtfile(self,gt_path,rescale_fac = 1.0):\n",
        "        coor_lists = list()\n",
        "        with open(gt_path,encoding='UTF-8') as f:\n",
        "            content = f.readlines()\n",
        "            for line in content:\n",
        "                coor_list = line.split(',')[:8]\n",
        "                if len(coor_list)==8:\n",
        "                    coor_lists.append(coor_list)\n",
        "        return self.box_transfer_v2(coor_lists,rescale_fac)\n",
        "\n",
        "    def draw_boxes(self,img,cls,base_anchors,gt_box):\n",
        "        for i in range(len(cls)):\n",
        "            if cls[i]==1:\n",
        "                pt1 = (int(base_anchors[i][0]),int(base_anchors[i][1]))\n",
        "                pt2 = (int(base_anchors[i][2]),int(base_anchors[i][3]))\n",
        "                img = cv2.rectangle(img,pt1,pt2,(200,100,100))\n",
        "        for i in range(gt_box.shape[0]):\n",
        "            pt1 = (int(gt_box[i][0]),int(gt_box[i][1]))\n",
        "            pt2 = (int(gt_box[i][2]),int(gt_box[i][3]))\n",
        "            img = cv2.rectangle(img, pt1, pt2, (100, 200, 100))\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img_path = os.path.join(self.datadir, img_name)\n",
        "        # print(img_path)\n",
        "        img = cv2.imread(img_path)\n",
        "        #####for read error, use default image#####\n",
        "        if img is None:\n",
        "            print(img_path)\n",
        "            with open('error_imgs.txt','a') as f:\n",
        "                f.write('{}\\n'.format(img_path))\n",
        "            img_name = 'img_2647.jpg'\n",
        "            img_path = os.path.join(self.datadir, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "        #####for read error, use default image#####\n",
        "\n",
        "        h, w, c = img.shape\n",
        "        rescale_fac = max(h, w) / 1600\n",
        "        if rescale_fac>1.0:\n",
        "            h = int(h/rescale_fac)\n",
        "            w = int(w/rescale_fac)\n",
        "            img = cv2.resize(img,(w,h))\n",
        "\n",
        "        gt_path = os.path.join(self.labelsdir, 'gt_'+img_name.split('.')[0]+'.txt')\n",
        "        gtbox = self.parse_gtfile(gt_path,rescale_fac)\n",
        "\n",
        "        # clip image\n",
        "        if np.random.randint(2) == 1:\n",
        "            img = img[:, ::-1, :]\n",
        "            newx1 = w - gtbox[:, 2] - 1\n",
        "            newx2 = w - gtbox[:, 0] - 1\n",
        "            gtbox[:, 0] = newx1\n",
        "            gtbox[:, 2] = newx2\n",
        "\n",
        "        [cls, regr], base_anchors = cal_rpn((h, w), (int(h / 16), int(w / 16)), 16, gtbox)\n",
        "        # debug_img = self.draw_boxes(img.copy(),cls,base_anchors,gtbox)\n",
        "        # cv2.imwrite('debug/{}'.format(img_name),debug_img)\n",
        "        m_img = img - IMAGE_MEAN\n",
        "\n",
        "        regr = np.hstack([cls.reshape(cls.shape[0], 1), regr])\n",
        "\n",
        "        cls = np.expand_dims(cls, axis=0)\n",
        "\n",
        "        # transform to torch tensor\n",
        "        m_img = torch.from_numpy(m_img.transpose([2, 0, 1])).float()\n",
        "        cls = torch.from_numpy(cls).float()\n",
        "        regr = torch.from_numpy(regr).float()\n",
        "\n",
        "        return m_img, cls, regr\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    xmin = 15\n",
        "    xmax = 95\n",
        "    for i in range(xmin//16+1,xmax//16+1):\n",
        "        print(16*i-0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgNLgzWVsJy-",
        "outputId": "eb693936-9c57-43c9-8798-2ea16280e72a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.5\n",
            "31.5\n",
            "47.5\n",
            "63.5\n",
            "79.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# dataset_download:https://rrc.cvc.uab.es/?ch=8&com=downloads\n",
        "random_seed = 2019\n",
        "torch.random.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "epochs = 30\n",
        "lr = 1e-3\n",
        "resume_epoch = 0\n",
        "\n",
        "\n",
        "def save_checkpoint(state, epoch, loss_cls, loss_regr, loss, ext='pth'):\n",
        "    check_path = os.path.join(checkpoints_dir,\n",
        "                              f'v3_ctpn_ep{epoch:02d}_'\n",
        "                              f'{loss_cls:.4f}_{loss_regr:.4f}_{loss:.4f}.{ext}')\n",
        "\n",
        "    try:\n",
        "        torch.save(state, check_path)\n",
        "    except BaseException as e:\n",
        "        print(e)\n",
        "        print('fail to save to {}'.format(check_path))\n",
        "    print('saving to {}'.format(check_path))\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda:0')\n",
        "    checkpoints_weight = pretrained_weights\n",
        "    print('exist pretrained ',os.path.exists(checkpoints_weight))\n",
        "    if os.path.exists(checkpoints_weight):\n",
        "        pretrained = False\n",
        "\n",
        "    dataset = ICDARDataset(icdar17_mlt_img_dir, icdar17_mlt_gt_dir)\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
        "    model = CTPN_Model()\n",
        "    model.to(device)\n",
        "\n",
        "    if os.path.exists(checkpoints_weight):\n",
        "        print('using pretrained weight: {}'.format(checkpoints_weight))\n",
        "        cc = torch.load(checkpoints_weight, map_location=device)\n",
        "        model.load_state_dict(cc['model_state_dict'])\n",
        "        resume_epoch = cc['epoch']\n",
        "    else:\n",
        "        model.apply(weights_init)\n",
        "\n",
        "    params_to_uodate = model.parameters()\n",
        "    optimizer = optim.SGD(params_to_uodate, lr=lr, momentum=0.9)\n",
        "\n",
        "    critetion_cls = RPN_CLS_Loss(device)\n",
        "    critetion_regr = RPN_REGR_Loss(device)\n",
        "\n",
        "    best_loss_cls = 100\n",
        "    best_loss_regr = 100\n",
        "    best_loss = 100\n",
        "    best_model = None\n",
        "    epochs += resume_epoch\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    for epoch in range(resume_epoch+1, epochs):\n",
        "        print(f'Epoch {epoch}/{epochs}')\n",
        "        print('#'*50)\n",
        "        epoch_size = len(dataset) // 1\n",
        "        model.train()\n",
        "        epoch_loss_cls = 0\n",
        "        epoch_loss_regr = 0\n",
        "        epoch_loss = 0\n",
        "        scheduler.step(epoch)\n",
        "\n",
        "        for batch_i, (imgs, clss, regrs) in enumerate(dataloader):\n",
        "            # print(imgs.shape)\n",
        "            imgs = imgs.to(device)\n",
        "            clss = clss.to(device)\n",
        "            regrs = regrs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            out_cls, out_regr = model(imgs)\n",
        "            loss_cls = critetion_cls(out_cls, clss)\n",
        "            loss_regr = critetion_regr(out_regr, regrs)\n",
        "\n",
        "            loss = loss_cls + loss_regr  # total loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss_cls += loss_cls.item()\n",
        "            epoch_loss_regr += loss_regr.item()\n",
        "            epoch_loss += loss.item()\n",
        "            mmp = batch_i+1\n",
        "\n",
        "            print(f'Ep:{epoch}/{epochs-1}--'\n",
        "                  f'Batch:{batch_i}/{epoch_size}\\n'\n",
        "                  f'batch: loss_cls:{loss_cls.item():.4f}--loss_regr:{loss_regr.item():.4f}--loss:{loss.item():.4f}\\n'\n",
        "                  f'Epoch: loss_cls:{epoch_loss_cls/mmp:.4f}--loss_regr:{epoch_loss_regr/mmp:.4f}--'\n",
        "                  f'loss:{epoch_loss/mmp:.4f}\\n')\n",
        "\n",
        "        epoch_loss_cls /= epoch_size\n",
        "        epoch_loss_regr /= epoch_size\n",
        "        epoch_loss /= epoch_size\n",
        "        print(f'Epoch:{epoch}--{epoch_loss_cls:.4f}--{epoch_loss_regr:.4f}--{epoch_loss:.4f}')\n",
        "        if best_loss_cls > epoch_loss_cls or best_loss_regr > epoch_loss_regr or best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_loss_regr = epoch_loss_regr\n",
        "            best_loss_cls = epoch_loss_cls\n",
        "            best_model = model\n",
        "            save_checkpoint({'model_state_dict': best_model.state_dict(),\n",
        "                             'epoch': epoch},\n",
        "                            epoch,\n",
        "                            best_loss_cls,\n",
        "                            best_loss_regr,\n",
        "                            best_loss)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m94Knuw7ss_H",
        "outputId": "98ee067e-301b-4fc7-d0fc-c31d233af9ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exist pretrained  True\n",
            "using pretrained weight: /content/sample_data/train_ctpn/checkpoints/v3_ctpn_ep24_0.6899_0.1993_0.8892.pth\n",
            "Epoch 25/54\n",
            "##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-04fb77bb6c53>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cc = torch.load(checkpoints_weight, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep:25/53--Batch:0/200\n",
            "batch: loss_cls:0.6962--loss_regr:0.1454--loss:0.8416\n",
            "Epoch: loss_cls:0.6962--loss_regr:0.1454--loss:0.8416\n",
            "\n",
            "Ep:25/53--Batch:1/200\n",
            "batch: loss_cls:0.6954--loss_regr:0.2026--loss:0.8980\n",
            "Epoch: loss_cls:0.6958--loss_regr:0.1740--loss:0.8698\n",
            "\n",
            "Ep:25/53--Batch:2/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1726--loss:0.8679\n",
            "Epoch: loss_cls:0.6956--loss_regr:0.1736--loss:0.8692\n",
            "\n",
            "Ep:25/53--Batch:3/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.2109--loss:0.9062\n",
            "Epoch: loss_cls:0.6955--loss_regr:0.1829--loss:0.8784\n",
            "\n",
            "Ep:25/53--Batch:4/200\n",
            "batch: loss_cls:0.6963--loss_regr:0.2077--loss:0.9041\n",
            "Epoch: loss_cls:0.6957--loss_regr:0.1879--loss:0.8836\n",
            "\n",
            "Ep:25/53--Batch:5/200\n",
            "batch: loss_cls:0.6954--loss_regr:0.1263--loss:0.8217\n",
            "Epoch: loss_cls:0.6956--loss_regr:0.1776--loss:0.8732\n",
            "\n",
            "Ep:25/53--Batch:6/200\n",
            "batch: loss_cls:0.6954--loss_regr:0.1936--loss:0.8890\n",
            "Epoch: loss_cls:0.6956--loss_regr:0.1799--loss:0.8755\n",
            "\n",
            "Ep:25/53--Batch:7/200\n",
            "batch: loss_cls:0.6862--loss_regr:0.0304--loss:0.7166\n",
            "Epoch: loss_cls:0.6944--loss_regr:0.1612--loss:0.8556\n",
            "\n",
            "Ep:25/53--Batch:8/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.1357--loss:0.8308\n",
            "Epoch: loss_cls:0.6945--loss_regr:0.1584--loss:0.8529\n",
            "\n",
            "Ep:25/53--Batch:9/200\n",
            "batch: loss_cls:0.6802--loss_regr:0.7183--loss:1.3984\n",
            "Epoch: loss_cls:0.6931--loss_regr:0.2144--loss:0.9074\n",
            "\n",
            "Ep:25/53--Batch:10/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1451--loss:0.8404\n",
            "Epoch: loss_cls:0.6933--loss_regr:0.2081--loss:0.9013\n",
            "\n",
            "Ep:25/53--Batch:11/200\n",
            "batch: loss_cls:0.6946--loss_regr:0.1728--loss:0.8674\n",
            "Epoch: loss_cls:0.6934--loss_regr:0.2051--loss:0.8985\n",
            "\n",
            "Ep:25/53--Batch:12/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.0594--loss:0.7544\n",
            "Epoch: loss_cls:0.6935--loss_regr:0.1939--loss:0.8874\n",
            "\n",
            "Ep:25/53--Batch:13/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1818--loss:0.8771\n",
            "Epoch: loss_cls:0.6936--loss_regr:0.1930--loss:0.8867\n",
            "\n",
            "Ep:25/53--Batch:14/200\n",
            "batch: loss_cls:0.6951--loss_regr:0.1409--loss:0.8360\n",
            "Epoch: loss_cls:0.6937--loss_regr:0.1896--loss:0.8833\n",
            "\n",
            "Ep:25/53--Batch:15/200\n",
            "batch: loss_cls:0.6956--loss_regr:0.1013--loss:0.7969\n",
            "Epoch: loss_cls:0.6939--loss_regr:0.1841--loss:0.8779\n",
            "\n",
            "Ep:25/53--Batch:16/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.1686--loss:0.8638\n",
            "Epoch: loss_cls:0.6939--loss_regr:0.1831--loss:0.8771\n",
            "\n",
            "Ep:25/53--Batch:17/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.1656--loss:0.8606\n",
            "Epoch: loss_cls:0.6940--loss_regr:0.1822--loss:0.8762\n",
            "\n",
            "Ep:25/53--Batch:18/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1516--loss:0.8469\n",
            "Epoch: loss_cls:0.6941--loss_regr:0.1806--loss:0.8746\n",
            "\n",
            "Ep:25/53--Batch:19/200\n",
            "batch: loss_cls:0.6870--loss_regr:0.1714--loss:0.8584\n",
            "Epoch: loss_cls:0.6937--loss_regr:0.1801--loss:0.8738\n",
            "\n",
            "Ep:25/53--Batch:20/200\n",
            "batch: loss_cls:0.6959--loss_regr:0.1690--loss:0.8649\n",
            "Epoch: loss_cls:0.6938--loss_regr:0.1796--loss:0.8734\n",
            "\n",
            "Ep:25/53--Batch:21/200\n",
            "batch: loss_cls:0.6960--loss_regr:0.1332--loss:0.8291\n",
            "Epoch: loss_cls:0.6939--loss_regr:0.1775--loss:0.8714\n",
            "\n",
            "Ep:25/53--Batch:22/200\n",
            "batch: loss_cls:0.6710--loss_regr:0.9240--loss:1.5950\n",
            "Epoch: loss_cls:0.6929--loss_regr:0.2099--loss:0.9028\n",
            "\n",
            "Ep:25/53--Batch:23/200\n",
            "batch: loss_cls:0.6951--loss_regr:0.1786--loss:0.8737\n",
            "Epoch: loss_cls:0.6930--loss_regr:0.2086--loss:0.9016\n",
            "\n",
            "Ep:25/53--Batch:24/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.1762--loss:0.8712\n",
            "Epoch: loss_cls:0.6931--loss_regr:0.2073--loss:0.9004\n",
            "\n",
            "Ep:25/53--Batch:25/200\n",
            "batch: loss_cls:0.6902--loss_regr:0.4039--loss:1.0941\n",
            "Epoch: loss_cls:0.6930--loss_regr:0.2149--loss:0.9079\n",
            "\n",
            "Ep:25/53--Batch:26/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.1851--loss:0.8801\n",
            "Epoch: loss_cls:0.6931--loss_regr:0.2138--loss:0.9068\n",
            "\n",
            "Ep:25/53--Batch:27/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.1795--loss:0.8747\n",
            "Epoch: loss_cls:0.6931--loss_regr:0.2126--loss:0.9057\n",
            "\n",
            "Ep:25/53--Batch:28/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.0044--loss:0.6997\n",
            "Epoch: loss_cls:0.6932--loss_regr:0.2054--loss:0.8986\n",
            "\n",
            "Ep:25/53--Batch:29/200\n",
            "batch: loss_cls:0.6620--loss_regr:0.0223--loss:0.6842\n",
            "Epoch: loss_cls:0.6922--loss_regr:0.1993--loss:0.8914\n",
            "\n",
            "Ep:25/53--Batch:30/200\n",
            "batch: loss_cls:0.6958--loss_regr:0.1378--loss:0.8337\n",
            "Epoch: loss_cls:0.6923--loss_regr:0.1973--loss:0.8896\n",
            "\n",
            "Ep:25/53--Batch:31/200\n",
            "batch: loss_cls:0.6955--loss_regr:0.2835--loss:0.9790\n",
            "Epoch: loss_cls:0.6924--loss_regr:0.2000--loss:0.8924\n",
            "\n",
            "Ep:25/53--Batch:32/200\n",
            "batch: loss_cls:0.6715--loss_regr:0.0388--loss:0.7103\n",
            "Epoch: loss_cls:0.6918--loss_regr:0.1951--loss:0.8868\n",
            "\n",
            "Ep:25/53--Batch:33/200\n",
            "batch: loss_cls:0.6718--loss_regr:0.1838--loss:0.8556\n",
            "Epoch: loss_cls:0.6912--loss_regr:0.1948--loss:0.8859\n",
            "\n",
            "Ep:25/53--Batch:34/200\n",
            "batch: loss_cls:0.6739--loss_regr:0.2246--loss:0.8984\n",
            "Epoch: loss_cls:0.6907--loss_regr:0.1956--loss:0.8863\n",
            "\n",
            "Ep:25/53--Batch:35/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.0965--loss:0.7915\n",
            "Epoch: loss_cls:0.6908--loss_regr:0.1929--loss:0.8837\n",
            "\n",
            "Ep:25/53--Batch:36/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.0957--loss:0.7909\n",
            "Epoch: loss_cls:0.6909--loss_regr:0.1902--loss:0.8811\n",
            "\n",
            "Ep:25/53--Batch:37/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.1813--loss:0.8765\n",
            "Epoch: loss_cls:0.6910--loss_regr:0.1900--loss:0.8810\n",
            "\n",
            "Ep:25/53--Batch:38/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1808--loss:0.8761\n",
            "Epoch: loss_cls:0.6911--loss_regr:0.1898--loss:0.8809\n",
            "\n",
            "Ep:25/53--Batch:39/200\n",
            "batch: loss_cls:0.6865--loss_regr:0.0557--loss:0.7422\n",
            "Epoch: loss_cls:0.6910--loss_regr:0.1864--loss:0.8774\n",
            "\n",
            "Ep:25/53--Batch:40/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.1787--loss:0.8740\n",
            "Epoch: loss_cls:0.6911--loss_regr:0.1862--loss:0.8773\n",
            "\n",
            "Ep:25/53--Batch:41/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1991--loss:0.8944\n",
            "Epoch: loss_cls:0.6912--loss_regr:0.1865--loss:0.8778\n",
            "\n",
            "Ep:25/53--Batch:42/200\n",
            "batch: loss_cls:0.6954--loss_regr:0.2072--loss:0.9026\n",
            "Epoch: loss_cls:0.6913--loss_regr:0.1870--loss:0.8783\n",
            "\n",
            "Ep:25/53--Batch:43/200\n",
            "batch: loss_cls:0.6821--loss_regr:0.4503--loss:1.1324\n",
            "Epoch: loss_cls:0.6911--loss_regr:0.1930--loss:0.8841\n",
            "\n",
            "Ep:25/53--Batch:44/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.1632--loss:0.8582\n",
            "Epoch: loss_cls:0.6912--loss_regr:0.1923--loss:0.8835\n",
            "\n",
            "Ep:25/53--Batch:45/200\n",
            "batch: loss_cls:0.6961--loss_regr:0.1814--loss:0.8775\n",
            "Epoch: loss_cls:0.6913--loss_regr:0.1921--loss:0.8834\n",
            "\n",
            "Ep:25/53--Batch:46/200\n",
            "batch: loss_cls:0.6961--loss_regr:0.2589--loss:0.9550\n",
            "Epoch: loss_cls:0.6914--loss_regr:0.1935--loss:0.8849\n",
            "\n",
            "Ep:25/53--Batch:47/200\n",
            "batch: loss_cls:0.6955--loss_regr:0.1894--loss:0.8849\n",
            "Epoch: loss_cls:0.6915--loss_regr:0.1934--loss:0.8849\n",
            "\n",
            "Ep:25/53--Batch:48/200\n",
            "batch: loss_cls:0.6952--loss_regr:0.1420--loss:0.8372\n",
            "Epoch: loss_cls:0.6916--loss_regr:0.1924--loss:0.8839\n",
            "\n",
            "Ep:25/53--Batch:49/200\n",
            "batch: loss_cls:0.6947--loss_regr:0.1410--loss:0.8357\n",
            "Epoch: loss_cls:0.6916--loss_regr:0.1914--loss:0.8830\n",
            "\n",
            "Ep:25/53--Batch:50/200\n",
            "batch: loss_cls:0.6953--loss_regr:0.1772--loss:0.8724\n",
            "Epoch: loss_cls:0.6917--loss_regr:0.1911--loss:0.8828\n",
            "\n",
            "Ep:25/53--Batch:51/200\n",
            "batch: loss_cls:0.6951--loss_regr:0.1337--loss:0.8288\n",
            "Epoch: loss_cls:0.6918--loss_regr:0.1900--loss:0.8817\n",
            "\n",
            "Ep:25/53--Batch:52/200\n",
            "batch: loss_cls:0.6956--loss_regr:0.1534--loss:0.8490\n",
            "Epoch: loss_cls:0.6918--loss_regr:0.1893--loss:0.8811\n",
            "\n",
            "Ep:25/53--Batch:53/200\n",
            "batch: loss_cls:0.6956--loss_regr:0.1596--loss:0.8552\n",
            "Epoch: loss_cls:0.6919--loss_regr:0.1887--loss:0.8806\n",
            "\n",
            "Ep:25/53--Batch:54/200\n",
            "batch: loss_cls:0.6950--loss_regr:0.1497--loss:0.8447\n",
            "Epoch: loss_cls:0.6920--loss_regr:0.1880--loss:0.8800\n",
            "\n",
            "Ep:25/53--Batch:55/200\n",
            "batch: loss_cls:0.6956--loss_regr:0.2099--loss:0.9055\n",
            "Epoch: loss_cls:0.6920--loss_regr:0.1884--loss:0.8804\n",
            "\n",
            "Ep:25/53--Batch:56/200\n",
            "batch: loss_cls:0.6717--loss_regr:0.1648--loss:0.8364\n",
            "Epoch: loss_cls:0.6917--loss_regr:0.1880--loss:0.8797\n",
            "\n",
            "/content/sample_data/train_ctpn/train_data/train_img/img_7077.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-04fb77bb6c53>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregrs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;31m# print(imgs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0b347500a72e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m#####for read error, use default image#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mrescale_fac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrescale_fac\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "T0dRQQBUuKTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWkgWgZkuMms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}